{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "robi-datathon-19-easy-01",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "_ZYxOI1mGNV1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Step 01. Install All Dependencies\n",
        "\n",
        "This installs Apache Spark 2.3.3, Java 8, Findspark library that makes it easy for Python to work on the given Big Data."
      ]
    },
    {
      "metadata": {
        "id": "MG5jn29qF91X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "89afdb0c-4cab-48af-83ea-0761e1755ecd"
      },
      "cell_type": "code",
      "source": [
        "#OpenJDK Dependencies for Spark\n",
        "apt-get install openjdk-8-jdk-headless -qq > /dev/null \n",
        "\n",
        "#Download Apache Spark\n",
        "wget -q http://apache.osuosl.org/spark/spark-2.3.3/spark-2.3.3-bin-hadoop2.7.tgz \n",
        "\n",
        "#Apache Spark and Hadoop Unzip\n",
        "tar xf spark-2.3.3-bin-hadoop2.7.tgz \n",
        "\n",
        "#FindSpark Install\n",
        "pip install -q findspark \n",
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K    100% |████████████████████████████████| 3.8MB 11.8MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 102kB 27.6MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 163kB 36.5MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 61kB 23.9MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 51kB 28.5MB/s \n",
            "\u001b[?25h  Building wheel for csvkit (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Building wheel for agate-excel (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Building wheel for agate-dbf (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Building wheel for agate-sql (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Building wheel for parsedatetime (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uE_C9VOSHOKy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 02. Set Environment Variables\n",
        "Set the locations where Spark and Java are installed based on your installation configuration. Double check before you proceed."
      ]
    },
    {
      "metadata": {
        "id": "qrOQoyMmHRPL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.3-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qY1GD4JslzUH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 03. ELT - Load the Data: Mega Cloud Access\n",
        "This is an alternative approach to load datasets from already stored in [**Mega Cloud**](https://mega.nz) cloud repository. You need to install the necessary packages and put the link URL of cloud to load the file from cloud directly."
      ]
    },
    {
      "metadata": {
        "id": "LCqsmO2fl_9Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "72ff9155-a67d-42fb-ada6-e2ddc1d943af"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.system('git clone https://github.com/jeroenmeulenaar/python3-mega.git')\n",
        "os.chdir('python3-mega')\n",
        "os.system('pip install -r requirements.txt')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "BAYD5dmomHgL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 04. ELT - Load the Data: Read Uploaded Dataset\n",
        "In this approach you can directly load the uploaded dataset downloaded fro Mega Cloud Infrastructure"
      ]
    },
    {
      "metadata": {
        "id": "CCWFVfBsmM03",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from mega import Mega\n",
        "os.chdir('../')\n",
        "m_revenue = Mega.from_ephemeral()\n",
        "m_revenue.download_from_url('https://mega.nz/#!1lJH3Q4K!N94-KRSyn22FPb0yxiVJgndjxUStdlfC2_prWDYI2f0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9iRVypEMHiDe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 06. Start a Spark Session\n",
        "This basic code will prepare to start a Spark session."
      ]
    },
    {
      "metadata": {
        "id": "vrW7H-rmHmFm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('ml-datathon19-easy01').master(\"local[4]\").getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vyv5qDPL2RCp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 06. Exploration - Data Schema View\n",
        "Now let's load the DataFrame and see the schema view of the Spark dataset"
      ]
    },
    {
      "metadata": {
        "id": "KPXZWq4X2RMV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "8cb4db1b-a768-4051-ef15-a51346c47921"
      },
      "cell_type": "code",
      "source": [
        "df = spark.read.csv('revenue.csv', header = True, inferSchema = True)\n",
        "df.printSchema()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- msisdn: string (nullable = true)\n",
            " |-- week_number: integer (nullable = true)\n",
            " |-- revenue_usd: double (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xChkf1Gf5DGC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 07. Exploration - Data Type Overview\n",
        "Now let's see data types of all available fields of the Spark dataset"
      ]
    },
    {
      "metadata": {
        "id": "8kIzQHQ35Cf1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c2a3e14b-9367-4f79-92a8-9d1d2acb5ab6"
      },
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('msisdn', 'string'), ('week_number', 'int'), ('revenue_usd', 'double')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "0QIDzJSb-4nh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 08. Exploration - More Statistical Insights from Data\n",
        "Now we'll grab total number of entries and other statistical analysis of the Spark dataset to have an overview of data"
      ]
    },
    {
      "metadata": {
        "id": "gSA4uWpd9-jL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "feaf8150-cecb-47c8-a2ba-23e5977c59d1"
      },
      "cell_type": "code",
      "source": [
        "df.describe().show() "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+--------------------+------------------+--------------------+\n",
            "|summary|              msisdn|       week_number|         revenue_usd|\n",
            "+-------+--------------------+------------------+--------------------+\n",
            "|  count|            28755967|          28755967|            28755967|\n",
            "|   mean|3.207318550755298E31|28.811598545790513|  11.326884020257502|\n",
            "| stddev|                 0.0| 4.043789537464958|  6.0085535870539735|\n",
            "|    min|00000312304d5ee32...|                22|5.158072668697356E-4|\n",
            "|    max|fffffdf74b6520955...|                35|   621.5236222433649|\n",
            "+-------+--------------------+------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LJWm-tno9gq0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 09. Exploration - Total Unique Row Count\n",
        "Now we'll grab total number of unique entries or unique row count of the Spark dataset to have an overview of duplicate data"
      ]
    },
    {
      "metadata": {
        "id": "2qF1rbIN7XTo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "322c208f-833f-46eb-ea50-d18ec492d5ff"
      },
      "cell_type": "code",
      "source": [
        "print(\"Unique Rows: \")\n",
        "df.distinct().count()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique Rows: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28755967"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "PZfgybnr95iA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 10. Implementation - Run the SQL Command\n",
        "Now since we got the idea that there is no NULL values and Duplicate rows, we can straightly go for executing SQL command to get the desired outcome. As a part of optimisation, we can drop of the column week_number as this is not relevant to this problem."
      ]
    },
    {
      "metadata": {
        "id": "EA037xxSLEfF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "b5f9d1f2-7597-49f3-b491-d0e7c6dc2653"
      },
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import desc\n",
        "df2 = df.drop(df.week_number)\n",
        "Easy01 = df2.groupBy('msisdn').agg({'revenue_usd':'sum'}).sort(desc(\"sum(revenue_usd)\"))\n",
        "Easy01 = Easy01.withColumnRenamed(\"sum(revenue_usd)\", \"TotalRevenue_USD\")\n",
        "Easy01 = Easy01.withColumnRenamed(\"msisdn\", \"User\")\n",
        "Easy01.show(n=5, truncate=False)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------------------+-----------------+\n",
            "|User                            |TotalRevenue_USD |\n",
            "+--------------------------------+-----------------+\n",
            "|711582466cb593b5c31a561e3fb8754f|918.0708735522023|\n",
            "|1d14aa68494db76eea8640ee2e4bfd32|910.5638393486429|\n",
            "|5313925c9e43ff9dbbb9a4ac9d5f75ef|861.8608024104357|\n",
            "|d08f4b304529a54d43b2dc5afba01811|763.6768256830246|\n",
            "|ef5dd3752d155c2bae352f7ed72f968d|744.4222285995457|\n",
            "+--------------------------------+-----------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
